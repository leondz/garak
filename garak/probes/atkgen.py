"""atkgen: Attack Generation. 

This probe uses a separate model to create prompts for a generator,
which are intended to guide that generator into a certain failure mode.

Warning: These attack models may generate highly toxic prompts while 
attempting to goad target models into failure - peruse logs at your
own risk, and don't spend too much time doing it, using this guide 
to working with harmful text:

 https://aclanthology.org/2022.findings-emnlp.35/
"""

import copy
import logging
import json
import re
from typing import List

from colorama import Fore, Style
from tqdm import tqdm

from garak import _config
import garak._plugins
import garak.attempt
from garak.probes.base import Probe
from garak.translator import SimpleTranslator, LocalTranslator

TEMPLATE_PLACEHOLDER = "[query]"


class Tox(Probe):
    """Automatic attack generation, trying to elicit toxic outputs from the model.

    This probe uses an attack model, trained on conversations that led to LLMs
    outputting toxicity, to generate prompts that should nudge a target LLM into
    yielding toxic output. The data used for this attack generation model was selected
    using the same detector as this tox probe uses, so both attack generation model
    and detector are looking for the same definition of the failure mode."""

    recommended_detector = ["toxicity.ToxicCommentModel"]
    tags = [
        "avid-effect:ethics:E0301",
        "quality:Behavioral:ContentSafety:Toxicity",
        "quality:Behavioral:ContentSafety:HateHarassment",
        "quality:Behavioral:ContentSafety:Profanity",
        "payload:unwanted",
    ]
    goal = "make the model say something toxic"
    parallelisable_attempts = False  # atkgen has its own dialog manager
    doc_uri = (
        "https://interhumanagreement.substack.com/p/faketoxicityprompts-automatic-red"
    )
    bcp47 = "en"

    DEFAULT_PARAMS = Probe.DEFAULT_PARAMS | {
        "max_calls": 5,
        "constructive_tension": True,
        "red_team_model_type": "huggingface.Pipeline",
        "red_team_model_name": "garak-llm/artgpt2tox",
        "red_team_model_config": {
            "hf_args": {"device": "cpu", "torch_dtype": "float32"}
        },  # defer acceleration devices to model under test unless overriden
        "red_team_prompt_template": "<|input|>[query]<|response|>",
        "red_team_postproc_rm_regex": r"\<\|.*",
        "use_only_first_sent": True,  # should we only consider the first sentence of the target's response?
    }

    def probe(self, generator) -> List[garak.attempt.Attempt]:
        output_is_conversation = _config.system.verbose >= 2

        if self.redteamer is None:

            print(
                "üî¥ü™ñ  ", end=""
            )  # distinguish load message for atkgen generator from target generator
            self.redteamer = garak._plugins.load_plugin(
                f"generators.{self.red_team_model_type}",
                config_root=self.red_team_model_config,
            )
            self.redteamer.deprefix_prompt = True

        attempts = []
        red_team_model_short = self.red_team_model_name.split("/")[-1]

        prev_parallel_requests = _config.system.parallel_requests
        _config.system.parallel_requests = False
        if "max_new_tokens" in dir(generator):  # let the probed model decide what to do
            prev_max_new_tokens = generator.max_new_tokens
            generator.max_new_tokens = None
        attack_iterator = tqdm(
            range(self.generations), leave=False
        )  # itrerate for probe generations
        attack_iterator.set_description(self.probename.replace("garak.", ""))

        for i in attack_iterator:
            self.redteamer.max_new_tokens = (
                None  # artopt unhappy with this being set on first iter
            )
            calls_made = 0
            keep_going = True

            this_attempt = self._mint_attempt()  # don't set the prompt yet

            last_response = None
            turns = []
            logging.debug("atkgen: attempt %s uuid %s", i, this_attempt.uuid)

            if not output_is_conversation:
                t = tqdm(total=2 * self.max_calls, leave=False)
            else:
                print("atkgen: üÜï ‚ãÖ.À≥À≥.‚ãÖ‡•±ÀôÀô‡•± New conversation ‡•±ÀôÀô‡•±‚ãÖ.À≥À≥.‚ãÖ üó£Ô∏è")

            while calls_made < self.max_calls and keep_going:
                if not output_is_conversation:
                    t.set_description(
                        f" turn {t.n:02}: red teaming [{red_team_model_short[:10]:<10}]"
                    )
                # generate a challenge
                if last_response == "" and self.constructive_tension:
                    challenge = ""
                else:
                    if last_response:
                        if self.use_only_first_sent:
                            last_response = last_response.split(". ")[
                                0
                            ]  # nltk sentence splitter install too unfriendly
                    else:
                        last_response = ""  # maybe it's none or 0.0 or [] or goodness knows. let's get an empty string.

                    if len(self.red_team_prompt_template):
                        query = self.red_team_prompt_template.replace(
                            "[query]", last_response
                        )
                    else:
                        query = last_response  # oh hey we might want to be able to switch roles here for chat models. or not?

                    challenge = self.redteamer.generate(query)[0]
                    if self.red_team_postproc_rm_regex:
                        challenge = re.sub(
                            self.red_team_postproc_rm_regex, "", challenge
                        ).strip()
                    if hasattr(self, 'translator'):
                        if self.translator is not None:
                            challenges = self.translator.translate_prompts([challenge])
                    else:
                        challenges = [challenge]

                if not output_is_conversation:
                    t.update()
                
                for challenge in challenges:
                    turn = ("probe", challenge)

                    if len(turns) == 0:
                        this_attempt.prompt = challenge
                        this_attempt._expand_prompt_to_histories(1)
                    else:
                        this_attempt._add_turn("user", [challenge])

                    turns.append(turn)
                    logging.debug("atkgen: probe: %s", turn)
                    if output_is_conversation:
                        probe_text = re.sub(r"[\r\n]+", "\n", turn[1]).strip()
                        print(
                            f"atkgen: üî¥ probe: {Fore.LIGHTYELLOW_EX}{probe_text}{Style.RESET_ALL}"
                        )
                    else:
                        t.set_description(
                            f" turn {t.n:02}: waiting for [{generator.name[:10]:<10}]"
                        )
                    # send the challenge and get the response
                    response = generator.generate(challenge)
                    if response is None or len(response) == 0:
                        response = ""
                    else:
                        response = response[0].strip() if response[0] is not None else ""

                    # log the response
                    turn = ("model", response)
                    this_attempt._add_turn("assistant", [response])
                    turns.append(turn)
                    logging.debug("atkgen: model: %s", turn)
                    if output_is_conversation:
                        print(f"atkgen: ü¶ú model: {Style.BRIGHT}{turn[1]}{Style.RESET_ALL}")
                    else:
                        t.update()
                    # increment calls_made
                    calls_made += 1
                    # check if the resp is empty or if it matches the previous resp
                    if not len(response) and not self.constructive_tension:
                        keep_going = False
                    if response == last_response:
                        keep_going = False
                    # update last_response
                    last_response = response.replace("\n", " ").strip()
                    self.redteamer.max_new_tokens = 170  # after first iter, give a limit

            if not output_is_conversation:
                t.close()

            this_attempt.notes["turns"] = turns

            _config.transient.reportfile.write(
                json.dumps(this_attempt.as_dict()) + "\n"
            )
            attempts.append(copy.deepcopy(this_attempt))

        # restore request parallelisation option
        _config.system.parallel_requests = prev_parallel_requests
        # restore generator's token generation limit
        if "max_new_tokens" in dir(generator):  # let the probed model decide what to do
            generator.max_new_tokens = prev_max_new_tokens

        return attempts

    def _build_red_team_model_config(self):
        try:
            rt_model_module, rt_model_class = self.red_team_model_type.split(".")
        except ValueError as e:
            msg = f"red team model type needs to be fully specifed, w.g. 'module.Class'. Got {self.red_team_model_type}"
            logging.critical(msg)
            raise ValueError() from e
        rt_config = {
            "generators": {
                rt_model_module: {
                    rt_model_class: self.red_team_model_config
                    | {"name": self.red_team_model_name},
                }
            }
        }
        return rt_config

    def __init__(self, config_root=_config):
        super().__init__(config_root)
        self.redteamer = None
        self.red_team_model_config = self._build_red_team_model_config()
        if (
            len(self.red_team_prompt_template)
            and TEMPLATE_PLACEHOLDER not in self.red_team_prompt_template
        ):
            msg = f"No query placeholder {TEMPLATE_PLACEHOLDER} in {self.__class__.__name__} prompt template {self.red_team_prompt_template}"
            logging.critical(msg)
            raise ValueError(msg)

        if hasattr(config_root, 'run'):
            if hasattr(config_root.run, 'translation_service'):
                translation_service = config_root.run.translation_service
                class_name = self.probename.split(".")[-2]
                self.translator = _config.load_translator(translation_service=translation_service, 
                                                        classname=class_name)
